{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    LSTM,\n",
    "    Dropout,\n",
    "    Lambda,\n",
    "    Flatten,\n",
    "    TimeDistributed\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from scipy.stats import moment\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import sklearn\n",
    "import math\n",
    "import plotly.express as px\n",
    "\n",
    "import time\n",
    "import math\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('train.csv')\n",
    "test_set = pd.read_csv('test.csv')\n",
    "train_set['date'] = pd.to_datetime(train_set['date'], format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert the notion of weekdays  and days of month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['year']    =  train_set['date'].dt.year\n",
    "train_set['month']   =  train_set['date'].dt.month\n",
    "train_set['day']     =  train_set['date'].dt.dayofyear\n",
    "train_set['weekday'] =  train_set['date'].dt.weekday\n",
    "train_set['weekend'] =  train_set['weekday'] > 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifing the correlation between weekendays and sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_set['weekday'].max())\n",
    "print(train_set['weekday'].min())\n",
    "\n",
    "for value in range(train_set['weekday'].min(),train_set['weekday'].max() + 1):\n",
    "    mean_weekday = train_set[(train_set['weekday'] == value)]['sales'].mean() \n",
    "    plt.bar([value],[mean_weekday], label=f'mean weekday{value}')\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel('Mean of sales value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This shows that the sales value increases in the weekends and decreces in the weekdays\n",
    "This means that the weekdays could be valuable income data in a machine learning model, this is possible to know beforehand what the weekdays of the desirable prediction and this data presents a direct correlation between the value of the sales. Thus, the `weekdays` value could be a good entry feature of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geting the series of one store and one item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_set = train_set[(train_set['item'] == 1) & (train_set['store'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_train_set = item_set[(item_set['date'] < \"2016-12-31\")]\n",
    "item_test_set  = item_set[(item_set['date'] > \"2016-12-31\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important analyses of time series data is to detect if the subject series is stationary or not, if not further transformations are required to remove trend and seasonality of the series, this analysis is even more important using statistic based models. Many authors of papers that use neural networks (NN) for time series defend that using this kind of machine learning model does not necessarily require the data to be already stationary. Although, is important to know if the data is stationary even if the model selected is a NN, because if the model does not converges well this could be a cause and know the properties of the series is highly valuable as well.\n",
    "\n",
    "[1] Define a weak stationary time series if the mean function $ E[x(t)] $ is independent of $ t $, if the autocoraviation function $Cov (x(t+h), x(t))$ is independent of $ t $ for each $h$ and if $E[x^2[n]]$ is finite for each $n$.\n",
    "\n",
    "To perform the weak stationary test, the mean function and the autocovariation function ware applied over rolling windows, since its sampled data. Thus, the window size has an impact over the functions interpretations, the window represents the interval in which the stationary hypothesis is tested.\n",
    "\n",
    "Besides this definition, the $ statsmodels $ library has the Augmented Dickey-Fuller unit root test. The Augmented Dickey-Fuller test can be used to test for a unit root in a univariate process in the presence of serial correlation.\n",
    "\n",
    "References\n",
    "[1] Brockwell, Peter J., and Richard A. Davis. Introduction to time series and forecasting. springer, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationary_test(entry,delta=200,ad=False,std=False):\n",
    "    window_size=int(len(entry)/15)\n",
    "    # Weak stationary test\n",
    "    # Mean function\n",
    "    mean_y = []\n",
    "    mean_x = []\n",
    "    \n",
    "    s_moment_y = []\n",
    "    std_y = []\n",
    "    n_data = len(entry)\n",
    "    for i in range(0, int(n_data - window_size)):\n",
    "        # Roling window start and end\n",
    "        n_start = i\n",
    "        n_end = n_start + window_size\n",
    "        # Mean, standard deviation and second moment calculation\n",
    "        mean_y_i = np.mean(entry[n_start:n_end])\n",
    "        s_moment_y_i = moment(entry[n_start:n_end],moment=2)\n",
    "        std_y_i = np.std(entry[n_start:n_end])\n",
    "        # Saving the results \n",
    "        mean_y.append(mean_y_i)\n",
    "        mean_x.append(n_end)\n",
    "        s_moment_y.append(s_moment_y_i)\n",
    "        std_y.append(std_y_i)\n",
    "\n",
    "    # Autocovariance function\n",
    "    acov_y = []\n",
    "    acov_x = []\n",
    "    n_data = len(entry)\n",
    "    for i in range(0, int(n_data - window_size - delta)):\n",
    "        n_start = i\n",
    "        n_end = n_start + window_size\n",
    "        acov_y_i = np.cov(\n",
    "            entry[n_start:n_end], entry[n_start+delta:n_end+delta]\n",
    "        )[0][0]\n",
    "        acov_y.append(acov_y_i)\n",
    "        acov_x.append(n_end)\n",
    "    if(ad):\n",
    "        result = adfuller(entry)\n",
    "        print(\"ADF Statistic: %f\" % result[0])\n",
    "        print(\"p-value: {0}\".format(result[1]))\n",
    "        print(\"Critical Values:\")\n",
    "        for key, value in result[4].items():\n",
    "            print(\"\\t%s: %.3f\" % (key, value))\n",
    "        # if the p-value < 0.05  and the adf statistic is less than\n",
    "        # critical values the series is stationary or is time independent\n",
    "        \n",
    "    return [mean_x,mean_y],[acov_x,acov_y], s_moment_y, std_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_train_set.plot(x='date', y='sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.autocorrelation_plot(item_train_set['sales'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Weak stationary test\n",
    "sales_train = item_train_set['sales'].to_numpy()\n",
    "\n",
    "mean, cov, s_moment, std = stationary_test(sales_train, delta=20,ad=True) \n",
    "\n",
    "plt.figure()\n",
    "plt.subplot()\n",
    "plt.plot(sales_train, \"b\", label=\"Sales\")\n",
    "plt.plot(mean[0], mean[1], \"r\", label=\"Mean function\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Amount of sales\")\n",
    "plt.grid('on')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sales_train, \"b\", label=\"Sales\")\n",
    "plt.plot(cov[0], cov[1], \"g\", label=\"autocovariance function\")\n",
    "plt.xlabel(\"samples index\")\n",
    "plt.ylabel(\"Covariance (ppb)\")\n",
    "plt.title(\"ACF da intensity of the price_test\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sales_train, \"b\", label=\"Sales\")\n",
    "plt.plot(mean[0], std, \"g\", label=\"standard deviation function\")\n",
    "plt.xlabel(\"samples index\")\n",
    "plt.ylabel(\"Amount of sales\")\n",
    "plt.title(\"Standard deviation function\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sales_train, \"b\", label=\"Sales\")\n",
    "plt.plot(mean[0], s_moment, \"g\", label=\"Second moment function\")\n",
    "plt.xlabel(\"samples index\")\n",
    "plt.ylabel(\"Amount of sales\")\n",
    "plt.title(\"Second moment function\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the test results\n",
    "The mean function test hypothesis failed, this can be noted by checking the function follows the trend and seasonlity of the sales series, the covariation function presents a linear behavior with a lot of disturbance, the standard deviation function seans to be linear like, but with a lot of noise. The second-moment function doesn't show a tendency to infinite on any $n$, that way passing on this condition.\n",
    "\n",
    "Analyzing the functions plots it's correct to infer that this series of sales is non-stationary, this can be also noted on the autocorrelation plot, which shows a slow decaying behavior.the autocorrelation plot shows also a periodic behavior, which represents a periodic behavior on the time series, and also in several lags the ACF value goes above and bellow the confidence intervals of 99% and 95%\n",
    "\n",
    "Besides the week sationary test, theAugmented Dickey-Fuller test can be done to check if the series of sales is stationary or not. This test checks if the series have a unit root and doing so it can make the assumption of how much the series is defined by it's trend.\n",
    "\n",
    "There are 2 Hypothesis:\n",
    "\n",
    "Null Hypothesis (H0): If failed to be rejected, it suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure.\n",
    "\n",
    "Alternate Hypothesis (H1): The null hypothesis is rejected; it suggests the time series does not have a unit root, meaning it is stationary. It does not have time-dependent structure.\n",
    "\n",
    "We interpret this result using the p-value from the test. A p-value below a threshold (such as 5% or 1%) suggests we reject the null hypothesis (stationary), otherwise a p-value above the threshold suggests we fail to reject the null hypothesis (non-stationary).\n",
    "\n",
    "p-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n",
    "p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.\n",
    "\n",
    "Since the p-value of the test value applied to the series is bigger than 0.05, the null hypothesis is rejected, thus the series is non-stationary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Neural networks for solving the time series problem\n",
    "First, it is necessary to transform this time series problem into a supervised learning problem; for this, it is necessary to define whether the network will make a recursive forecast, in which the forecasts will be used as resources for a longer time horizon or if the network will make a direct forecast, in which the network only uses the last steps of time and resources known as the day of the week, to forecast several steps.\n",
    "\n",
    "### Direct prediction approach\n",
    "To predict the 3 months of sales is necessary to define first the labels and features of the model, as features the model will use 3 months of past sales data and the 3 months of weekdays value. As labels would be 3 months of the future data, to extrat the labels and features of the data a rolling window approach is reaquired, this way a window of 3 months past data, 3 months of weekdays value could be related to 3 months of future data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling window approach\n",
    "\n",
    "With this approach is possible to turn a series of data into a supervised learning problem, this way the model can intepretate the problem properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the series of data needed and normalizing\n",
    "weekday_train = item_train_set['weekday'].to_numpy()/item_train_set['weekday'].max()\n",
    "sales_train   = item_train_set['sales'].to_numpy()\n",
    "index_val     = np.arange(len(sales_train))\n",
    "\n",
    "# Three months of prediction\n",
    "label_win = 90\n",
    "feat_win  = 90\n",
    "win_size  = label_win + feat_win \n",
    "n_windows = len(sales_train) - win_size\n",
    "\n",
    "x_train = np.zeros((n_windows, int(feat_win)))\n",
    "x_id = np.zeros((n_windows, int(feat_win)))\n",
    "y_train = np.zeros((n_windows, label_win))\n",
    "y_id = np.zeros((n_windows, label_win))\n",
    "\n",
    "for win in range(n_windows):\n",
    "    # Feature window start and end index\n",
    "    i_s_feat = win\n",
    "    i_e_feat = i_s_feat + feat_win\n",
    "    \n",
    "    # Label window start and end index\n",
    "    i_s_label = i_e_feat\n",
    "    i_e_label = i_s_label + label_win\n",
    "\n",
    "    # Geting the past sales feature\n",
    "    x_train[win] = sales_train[i_s_feat : i_e_feat] \\\n",
    "                   / item_train_set['sales'].max()\n",
    "    x_id[win] = index_val[i_s_feat : i_e_feat]\n",
    "    # Geting the weekdays feature\n",
    "    # x_train[win][feat_win:] = weekday_train[i_s_label : i_e_label]\n",
    "    \n",
    "    # Since we want the weekdays to be corralated to the label values\n",
    "    # the window of weekdays value that we will use will be the label's\n",
    "    # weekday. This makes sense taking in count that we know the weekdays\n",
    "    # beforehand\n",
    "\n",
    "    # Geting the labels\n",
    "    y_train[win] = sales_train[i_s_label:i_e_label]\n",
    "    y_id[win] = index_val[i_s_label:i_e_label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_train[90], label='features')\n",
    "plt.ylabel('Normalized value')\n",
    "plt.xlabel('Feature window index')\n",
    "plt.title(\"One sample of the model's features\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(y_train[0], label='labels')\n",
    "plt.ylabel('Sales amount')\n",
    "plt.xlabel('label index')\n",
    "plt.title(\"One sample of the model's Labels\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "The recommended neural network for a time series problem usualy is a LSTM, in this example peharps we will use a simple multi layer perceptron (MLP) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sales = item_train_set['sales'].max()\n",
    "entry_test = np.zeros(int(feat_win))\n",
    "entry_test[:feat_win] = item_train_set['sales'][-feat_win:].to_numpy() / max_sales \n",
    "# entry_test[feat_win:] = item_test_set['weekday'][:feat_win].to_numpy()\n",
    "\n",
    "label_test = item_test_set['sales'][:label_win].to_numpy()\n",
    "dates = item_test_set['date'][:label_win].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    # define nn\n",
    "    def __init__(self,input_shape, output_shape):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.fc3 = nn.Linear(100, output_shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = self.fc2(X)\n",
    "        X = self.fc3(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def predict(self, input_n):\n",
    "        \n",
    "        return self(test_X).cpu().detach().numpy()[0]\n",
    "    \n",
    "\n",
    "# wrap up with Variable in pytorch\n",
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "train_X = Variable(torch.Tensor(x_train).float()).to(dev)\n",
    "test_X  = Variable(torch.Tensor([entry_test]).float()).to(dev)\n",
    "train_y = Variable(torch.Tensor(y_train).float()).to(dev)\n",
    "test_y  = Variable(torch.Tensor([label_test]).float()).to(dev)\n",
    "net = Net(label_win,feat_win)\n",
    "net.to(dev) # is on cuda (all parameters)\n",
    "criterion = nn.MSELoss()# cross entropy loss\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100000):\n",
    "    optimizer.zero_grad()\n",
    "    out = net(train_X)\n",
    "    loss = criterion(out, train_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('number of epoch', epoch, 'loss', loss.item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = net.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df ={'date': dates,\n",
    "          'pred': prediction,\n",
    "          'label': label_test,\n",
    "          'error': prediction-label_test}\n",
    "\n",
    "fig = px.line(plot_df, x='date', y=['pred','label','error'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
